{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxM8hEuz6Xdc",
        "outputId": "f060e101-c402-427e-be86-e59c1d2bb7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umegyp4BjlHA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "\n",
        "def Th_comp_matmul(Ar, Ai, Br, Bi):  # Complex matmul pytorch function ########\n",
        "    if Ar.ndim == 3 and Br.ndim == 3:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, 0:int(c_th.shape[1] / 2), 0:int(c_th.shape[2] / 2)]\n",
        "        c_th_i = c_th[:, int(c_th.shape[1] / 2):, 0:int(c_th.shape[2] / 2)]\n",
        "    elif Ar.ndim == 2 and Br.ndim == 2:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=1), torch.cat((Ai, Ar), dim=1)), dim=0)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=1), torch.cat((Bi, Br), dim=1)), dim=0)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[0:int(c_th.shape[0] / 2), 0:int(c_th.shape[1] / 2)]\n",
        "        c_th_i = c_th[int(c_th.shape[0] / 2):, 0:int(c_th.shape[1] / 2)]\n",
        "    elif Ar.ndim == 4 and Br.ndim == 4:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "        c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim == 5 and Br.ndim == 5:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "        c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 12:\n",
        "        if Ar.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "          \n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "        elif Br.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 20:\n",
        "        if Ar.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "        elif Br.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    else:\n",
        "        raise Exception('the dimension is not defined for Th_comp_matmul.')\n",
        "\n",
        "    return c_th_r, c_th_i\n",
        "\n",
        "def Th_inv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    Ar_inv = torch.inverse(Ar + torch.matmul(torch.matmul(Ai, torch.inverse(Ar)), Ai))\n",
        "    Ai_inv = - torch.matmul(torch.matmul(torch.inverse(Ar), Ai), Ar_inv)\n",
        "    return Ar_inv, Ai_inv\n",
        "\n",
        "def Th_pinv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    if Ar.ndim == 2:\n",
        "        if Ar.shape[0] < Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.T, -Ai.T)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.T, -Ai.T, Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[0] > Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.T, -Ai.T, Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.T, -Ai.T)\n",
        "        elif Ar.shape[0] == Ar.shape[1]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 3:\n",
        "        if Ar.shape[1] < Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[1] > Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "        elif Ar.shape[1] == Ar.shape[2]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 4:\n",
        "        if Ar.shape[2] < Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[2] > Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "        elif Ar.shape[2] == Ar.shape[3]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    else:\n",
        "        raise Exception('5-D is not defined for Th_pinv.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ssl import CHANNEL_BINDING_TYPES\n",
        "import torch.utils.data as data\n",
        "from termcolor import colored\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.decomposition import FastICA\n",
        "# Database ####################################################################################################################\n",
        "class Data_Reader(data.Dataset):\n",
        "    def __init__(self, filename, Us, Mr, Nrf, K):\n",
        "\n",
        "        print(colored('You select core dataset', 'cyan'))\n",
        "        print(colored(filename, 'yellow'), 'is loading ... ')\n",
        "        np_data = np.load(filename)\n",
        "        #file3=np.load('/content/drive/MyDrive/HBF-Net-main/dataset/DataBase_pro_channel_BS32_2p4GHz_1Path.npy')\n",
        "        transformer = IncrementalPCA(n_components=128, batch_size=500)     \n",
        "        RSSI_N1 = np_data[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        #RSSI_N1 = file3[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        X_sparse = sparse.csr_matrix(RSSI_N1)\n",
        "        np_data2 = transformer.fit_transform(X_sparse)\n",
        "\n",
        "\n",
        "        #pca = PCA(n_components=128, svd_solver='full')\n",
        "        #pca = PCA(n_components=1, svd_solver='arpack')\n",
        "        #transformer = SparsePCA(n_components=128, random_state=0)\n",
        "        #transformer = KernelPCA(n_components=128, kernel='linear')\n",
        "        #np_data2 = transformer.fit_transform(RSSI_N1)\n",
        "        #np_data2= pca.fit_transform(RSSI_N1)\n",
        "        #model = NMF(n_components=128, init='random', random_state=0)\n",
        "        #np_data2 = model.fit_transform(RSSI_N1)\n",
        "        #transformer = FactorAnalysis(n_components=128, random_state=0)\n",
        "        #X_transformed = transformer.fit_transform(RSSI_N1)\n",
        "        #transformer = FastICA(n_components=128,\n",
        "                              #random_state=0,\n",
        "                              #whiten='unit-variance')\n",
        "        #np_data2 = transformer.fit_transform(RSSI_N1)\n",
        "        #model = NMF(n_components=128, init='random', random_state=0)\n",
        "        #np_data3 = model.fit_transform(model)       \n",
        "        #transformer = KernelPCA(n_components=128, kernel='linear')\n",
        "        #np_data3 = transformer.fit_transform(np_data2)\n",
        "\n",
        "\n",
        "        self.channelR = np_data[:, 0:Us * Mr].real.astype(float)\n",
        " \n",
        "        self.channelI = np_data[:, 0:Us * Mr].imag.astype(float)\n",
        "\n",
        "        self.alpha = np_data[:, Us * Mr: (Us * Mr) + (Us * K)].real.astype(float)\n",
        "\n",
        "        #self.RSSI_N = np_data[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        self.RSSI_N = np_data2\n",
        "        #self.RSSI_N1 = np_data2[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        self.UR = np_data[:, (Us * Mr) + (2 * Us * K):(2 * Us * Mr) + \n",
        "                          (2 * Us * K)].real.astype(float)\n",
        "        self.UI = np_data[:, (Us * Mr) + (2 * Us * K):(2 * Us * Mr) + \n",
        "                          (2 * Us * K)].imag.astype(float)\n",
        "\n",
        "        self.AR = np_data[:, Us * (2 * Mr + 2 * K):Us * (2 * Mr + 2 * K) + \n",
        "                          (Nrf * Mr)].real.astype(float)\n",
        "        self.AI = np_data[:, Us * (2 * Mr + 2 * K):Us * (2 * Mr + 2 * K) + \n",
        "                          (Nrf * Mr)].imag.astype(float)\n",
        "\n",
        "        self.target = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr):Us * (2 * Mr + 2 * K) + \n",
        "                              (Nrf * Mr) + 1].real.astype(int)\n",
        "\n",
        "        self.WR = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr) + 1:Us * \n",
        "                          (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1].real.astype(float)\n",
        "        self.WI = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr) + 1:Us * \n",
        "                          (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1].imag.astype(float)\n",
        "\n",
        "        self.deltaR = np_data[:, Us * (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1:Us * \n",
        "                              (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1].real.astype(float)\n",
        "        self.deltaI = np_data[:, Us * (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1:Us * \n",
        "                              (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1].imag.astype(float)\n",
        "\n",
        "        self.userp = np_data[:, Us * (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1: Us * \n",
        "                             (2 * Mr + 3 * K + Nrf + 2) + (Nrf * Mr) + 1].real.astype(float)\n",
        "\n",
        "        self.n_samples = np_data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def uniq_clas(self):\n",
        "        uniq = np.unique(self.target, return_counts=True)\n",
        "        NO_Class = np.unique(self.target).shape[0]\n",
        "        print(colored(\"The number of Unique AP in I1: \", \"green\"), NO_Class)\n",
        "        return np.max(uniq[1]) * 100 / uniq[1].sum()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(self.channelR[index]), torch.Tensor(self.channelI[index]), torch.Tensor(self.alpha[index]), \\\n",
        "            torch.Tensor(self.RSSI_N[index]), torch.Tensor(self.UR[index]), torch.Tensor(self.UI[index]), torch.Tensor(self.AR[index]), \\\n",
        "                torch.Tensor(self.AI[index]), torch.LongTensor(self.target[index]), torch.Tensor(self.WR[index]), torch.Tensor(self.WI[index]), \\\n",
        "                    torch.Tensor(self.deltaR[index]), torch.Tensor(self.deltaI[index]), torch.Tensor(self.userp[index])\n",
        "\n",
        "# readme reader for HBF initial parameters ####################################################################################\n",
        "def md_reader(DB_name):\n",
        "    md = genfromtxt('DATASET.md', delimiter='\\n', dtype='str')\n",
        "    Us = int(re.findall(r'\\d+', md[1])[0])\n",
        "    Mr = int(re.findall(r'\\d+', md[2])[0])\n",
        "    Nrf = int(re.findall(r'\\d+', md[3])[0])\n",
        "    Ass_n = int(re.findall(r'\\d+', md[4])[0])\n",
        "    Noise_pwr = float(''.join(('1e-', str(int(int(re.findall(r'\\d+', md[6])[0]) / 10)))))\n",
        "    return Us, Mr, Nrf, Ass_n, Noise_pwr\n",
        "\n",
        "class Initialization_Model_Params(object):\n",
        "    def __init__(self,\n",
        "                 DB_name,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids\n",
        "                 ):\n",
        "        self.DB_name = DB_name\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "\n",
        "    def Data_Load(self):\n",
        "        DataBase = Data_Reader(''.join(('DataBase_', self.DB_name, '.npy')),\n",
        "                               self.Us, self.Mr, self.Nrf, self.K)\n",
        "        uniq_dis_label = DataBase.uniq_clas()\n",
        "        return DataBase, uniq_dis_label\n",
        "\n",
        "    def Code_Read(self):\n",
        "        codes = genfromtxt('/content/drive/MyDrive/HBF-Net-main/Codebook/Codebook_ij.csv', delimiter=',', dtype='complex', skip_header=0)\n",
        "        #codes = genfromtxt('/content/drive/MyDrive/HBF-Net-main/Codebook/newexample2.csv', delimiter=',', dtype='complex', skip_header=0)\n",
        "        label = np.arange(len(codes))\n",
        "        self.n_output_clas = len(codes)\n",
        "        print(colored(\"The length of the codebook: \", \"green\"), len(codes))\n",
        "        Codes_idx = np.concatenate((label[:, np.newaxis], codes), axis=1)\n",
        "        codeword_C = {}\n",
        "        index_C = []\n",
        "        for i in range(len(codes)):\n",
        "            index_C = Codes_idx[i, 0].real.astype(int)\n",
        "            icode_C = Codes_idx[i, 1:]\n",
        "            codeword_C[index_C] = icode_C\n",
        "\n",
        "        # torch tensor of codes\n",
        "        codesr = torch.from_numpy(codes.real).type(torch.float)\n",
        "        codesi = torch.from_numpy(codes.imag).type(torch.float)\n",
        "        return codeword_C, len(codes), codesr, codesi\n",
        "\n",
        "class Loss_FDP_Rate_Based(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_FDP_Rate_Based, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=1, dim2=2)\n",
        "\n",
        "        SINR = diag_W / (torch.sum(W, 2) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(1)\n",
        "\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return -sum_rate.mean()\n",
        "\n",
        "    def evaluate_rate(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return sum_rate.mean()\n",
        "\n",
        "class Loss_HBF_Rate_Based_4D(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_HBF_Rate_Based_4D, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator_4d(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=2, dim2=3)\n",
        "        SINR = diag_W / (torch.sum(W, 3) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(2)\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2),\n",
        "                                            Ai.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2), Wr, Wi)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(2) ** 2 + HBF_prei.flatten(2) ** 2, dim=2))\n",
        "        HBF_prer = (HBF_prer.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_HBF_Rate_Based_4D.rate_calculator_4d(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "        return sum_rate.T\n",
        "\n",
        "    def evaluate_rate(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, self.Nrf, self.Mr).permute(0, 2, 1),\n",
        "            Ai.view(-1, self.Nrf, self.Mr).permute(0, 2, 1), Wr.permute(0, 2, 1), Wi.permute(0, 2, 1))\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(1) ** 2 + HBF_prei.flatten(1) ** 2, dim=1))\n",
        "        HBF_prer = (HBF_prer.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "\n",
        "        return sum_rate.mean()\n",
        "\n",
        "def FLP_loss(x, y):\n",
        "    log_prob = - 1.0 * F.softmax(x, 1)\n",
        "    #log_prob =  1.0 * F.log_softmax(x,1)\n",
        "    #log_prob = - 1.0 * F.softplus(x, 1)\n",
        "    temp = log_prob * y\n",
        "    cel = temp.sum(dim=1)\n",
        "    cel = cel.mean()\n",
        "    return cel\n"
      ],
      "metadata": {
        "id": "MrjpZv0djsuP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Net_m_task_CNN(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden, n_out_Reg, n_out_clas, p_dropout, U, Ass, out_channel, kernel_s, padding):\n",
        "        super(Net_m_task_CNN, self).__init__()\n",
        "\n",
        "        #self.cnn1 = ComplexConv2D(4,4,in_channels=1, out_channels=16, stride=1, padding=padding)\n",
        "        \n",
        "\n",
        "        #self.cnn1 = (ComplexConv2D(4, 4, strides=1, padding='valid',\n",
        "                          #kernel_initializer='complex_independent',name='Conv_P'))(encoded)\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do1 = nn.Dropout2d(p_dropout)\n",
        "        #self.do1 = nn.AlphaDropout(p_dropout)\n",
        "        #self.do1 = nn.FeatureAlphaDropout(0.2)\n",
        "\n",
        "\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "        self.do2 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do3 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu4 = nn.LeakyReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do4 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "\n",
        "        self.cnn5 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu5 = nn.LeakyReLU()\n",
        "        self.bn5 = nn.BatchNorm2d(num_features=8)\n",
        "        self.do5 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.do6 = nn.Dropout(0.05)\n",
        "        \n",
        "        self.rnn1 = nn.RNN(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.drnn1 = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.rnn2 = nn.RNN(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.drnn2 = nn.Dropout(0.05)\n",
        "\n",
        "        self.gru1 = nn.GRU(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.dgru1 = nn.Dropout(0.05)\n",
        "\n",
        "        self.gru2 = nn.GRU(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.dgru2 = nn.Dropout(0.05)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.do7 = nn.Dropout(0.05)\n",
        "\n",
        "        #self.lstm3 = nn.LSTM(32, n_in,\n",
        "                            #bidirectional=True, batch_first=True)\n",
        "        \n",
        "        #self.do6 = nn.Dropout(p_dropout)\n",
        "#end\n",
        "\n",
        "\n",
        "        # Fully connected 1 (readout)\n",
        "        x_new = (U + 2 * padding - kernel_s) + 1\n",
        "        y_new = (Ass + 2 * padding - kernel_s) + 1\n",
        "\n",
        "        nn_in_fc = 8 * (x_new + 2 * padding - kernel_s + 1) * (y_new + 2 * padding - kernel_s + 1)\n",
        "\n",
        "\n",
        "        self.fc20 = nn.Linear(nn_in_fc, n_hidden)\n",
        "        self.bn20 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu20 = nn.LeakyReLU()\n",
        "        self.do20 = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.fc30 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.bn30 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu30 = nn.LeakyReLU()\n",
        "        self.do30 = nn.Dropout(p_dropout)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc7R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc8R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc9C = nn.Linear(n_hidden, n_out_clas)\n",
        "\n",
        "    def forward(self, x):  # always\n",
        "\n",
        "        out = self.cnn1(x)\n",
        "        out = self.do1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "       # out = self.cnn2(out)\n",
        "        #out = self.do2(out)\n",
        "       # out = self.bn2(out)\n",
        "        #out = self.relu2(out)\n",
        "\n",
        "        ##out = self.cnn3(out)\n",
        "        #out = self.do3(out)\n",
        "        ##out = self.bn3(out)\n",
        "        #out = self.relu3(out)\n",
        "\n",
        "        out = self.cnn4(out)\n",
        "        out = self.do4(out)\n",
        "        out = self.bn4(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.cnn5(out)\n",
        "        out = self.do5(out)\n",
        "        out = self.bn5(out)\n",
        "        out = self.relu5(out)\n",
        "\n",
        "        #m6\n",
        "        #out= torch.flatten(out)\n",
        "        #out= out[0:131072]\n",
        "        #out = out.view(-1,1024)\n",
        "        out = out.view(out.size(0), -1)\n",
        "       # out = out.view(-1,64)\n",
        "        out = self.lstm1(out)\n",
        "        out = self.do6(out[0])\n",
        "        #out = self.gru1(out)\n",
        "        #out = self.dgru1(out[0]) \n",
        "\n",
        "        #out = self.rnn1(out)\n",
        "        #out = self.drnn1(out[0])\n",
        "        #out = self.lstm1(out)\n",
        "        #out = self.do4(out[0])\n",
        "        #out = out.view(-1,512)\n",
        "       # out = self.rnn2(out)\n",
        "        #out = self.drnn2(out[0])        \n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "       # out = self.lstm2(out)\n",
        "        #out = self.do7(out[0])  \n",
        "        #out = out.view(-1,512)\n",
        "\n",
        "       # out = self.gru1(out)\n",
        "        #out = self.dgru1(out[0]) \n",
        "       # out = self.lstm1(out)\n",
        "       # out = self.do4(out[0])\n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "        #out = self.gru2(out)\n",
        "        #out = self.dgru2(out[0]) \n",
        "        #out = out.view(-1,1024)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.fc20(out)\n",
        "        out = self.do20(out)\n",
        "        out = self.relu20(out)\n",
        "        out = self.bn20(out)\n",
        "\n",
        "        out = self.fc30(out)\n",
        "        out = self.do30(out)\n",
        "        out = self.relu30(out)\n",
        "        out = self.bn30(out)\n",
        "        #out= torch.flatten(out)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "        #out= out[1:131073]\n",
        "        #out = out.view(-1, 1024)\n",
        "        #out = self.rnn1(out)\n",
        "        #out = self.drnn1(out[0])\n",
        "        #out = out.view(-1,512)\n",
        "       # out = out.view(out.size(0), -1)\n",
        "        #out = self.rnn2(out)\n",
        "        #out = self.drnn2(out[0])        \n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "        #out = self.lstm2(out)\n",
        "        #out = self.do5(out[0]) \n",
        "        #out = self.gru2(out)\n",
        "        #out = self.dgru2(out[0])  \n",
        "\n",
        "        #out = self.lstm3(out)\n",
        "        #out = self.do6(out)     \n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,1024)\n",
        "        #out = self.lstm1(out)\n",
        "        #out = self.do6(out[0])\n",
        "       # out = out.view(out.size(0), -1)\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outR1 = self.fc7R(out)\n",
        "\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        #outR2 = self.fc8R(out)\n",
        "        outR2 = self.fc8R(out)\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outC = self.fc9C(out)\n",
        "\n",
        "        return outR1 ,outR2, outC"
      ],
      "metadata": {
        "id": "_k39ZKCXjuWz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import RegressorMixin\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "\n",
        "\n",
        "class Networks_activations(object):\n",
        "    def __init__(self,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids,\n",
        "                 n_input,\n",
        "                 n_hidden,\n",
        "                 n_output_reg,\n",
        "                 n_output_clas,\n",
        "                 p_dropout,\n",
        "                 out_channel,\n",
        "                 kernel_s,\n",
        "                 padding\n",
        "                 ):\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output_reg = n_output_reg\n",
        "        self.n_output_clas = n_output_clas\n",
        "        self.p_dropout = p_dropout\n",
        "        self.out_channel = out_channel\n",
        "        self.kernel_s = kernel_s\n",
        "        self.padding = padding\n",
        "        \n",
        "    def Network_m_Task(self):\n",
        "        if self.device.type == 'cuda':\n",
        "            return nn.DataParallel(Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas, self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding), device_ids=self.dev_id).to(self.device)\n",
        "        else:\n",
        "            return Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas,\n",
        "                self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding)\n",
        "\n",
        "    def Inp_MT(self, RSSI):       \n",
        "        Inputs_MT = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)\n",
        "        return Inputs_MT"
      ],
      "metadata": {
        "id": "b6_mt2W1j7Vm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy import genfromtxt\n",
        "import torch as th\n",
        "import time\n",
        "from termcolor import colored\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "###############################################################################\n",
        "# Directory file\n",
        "###############################################################################\n",
        "DB_name = 'dataSet64x8x4_130dB_0129201820'\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Processor selection GPU if available (using GPU is highly recommended)\n",
        "###############################################################################\n",
        "device = th.device(\"cuda:2\" if th.cuda.is_available() else \"cpu\")\n",
        "device_ids = [2, 1, 3]\n",
        "print(\"Is Cuda available? \", colored('True', 'green')\n",
        "    if th.cuda.is_available() else colored('False', 'red'))\n",
        "print(\"Which devide?\", colored(device, 'cyan'))\n",
        "\n",
        "###############################################################################\n",
        "# Setup Parameters\n",
        "###############################################################################\n",
        "\n",
        "# Beamforming approach  AFP_Net, HBF_NET   ####################################\n",
        "BF_approach = 'HCL'\n",
        "\n",
        "###############################################################################\n",
        "# Beamfroming system model and DNN Parameters\n",
        "###############################################################################\n",
        "#os.chdir(os.path.dirname(os.path.abspath('C:/Users/user/Desktop/dataSet64x8x4_130dB_0129201820.npy')))\n",
        "#os.chdir(os.path.dirname(os.path.abspath('/Desktop/DataBase_dataSet64x8x4_130dB_0129201820.npy')))\n",
        "os.chdir(os.path.dirname(os.path.abspath('/content/drive/MyDrive/HBF-Net-main/dataSet64x8x4_130dB_0129201820.npy')))\n",
        "Us, Mr, Nrf, K, Noise_pwr = md_reader(DB_name)                # Number of users, antenna, K, RF chains and noise power\n",
        "#m6\n",
        "#Us=8\n",
        "#Mr=128\n",
        "#Noise_pwr =120\n",
        "#K=\n",
        "#end\n",
        "\n",
        "\n",
        "K_limited = K                                                 # Number of SS as RSSI\n",
        "batch_size = 1024                                         # Batch size\n",
        "epoch_size = 100                                         # Number of training epoches\n",
        "#lr = 0.001 \n",
        "lr = 0.001                                                # Learning rate\n",
        "#wd = 1e-6 \n",
        "wd = 1e-9                                                      # Weight decay\n",
        "n_input = Us * K_limited                                      # Input dimensions\n",
        "n_hidden = 1024                                         # Size of FCL layers\n",
        "out_channel = 16                                              # Size of CL channels\n",
        "kernel_s = 3                                                  # Size of Kernels in CL\n",
        "padding = 1                                                   # Size of padding in CL\n",
        "p_dropout = 0.05                                              # Probability of dropout\n",
        "\n",
        "\n",
        "\n",
        "if BF_approach == 'HCL':\n",
        "    n_output_reg = Us * Nrf\n",
        "\n",
        "else:\n",
        "    raise Exception('BF_approach value is wrong !!')\n",
        "\n",
        "Main_Menu = Initialization_Model_Params(DB_name,\n",
        "                                        Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids)\n",
        "\n",
        "\n",
        "# Reading Database\n",
        "\n",
        "DataBase, uniq_dis_label = Main_Menu.Data_Load()\n",
        "\n",
        "# Codeword dictionary\n",
        "\n",
        "#codeword_C, n_output_clas, codesr, codesi = Main_Menu.Code_Read()\n",
        "codeword_C, n_output_clas, codesr, codesi = Main_Menu.Code_Read()\n",
        "\n",
        "# Training-set and test-set generation\n",
        "\n",
        "train_size = int(0.85 * len(DataBase))\n",
        "test_size = len(DataBase) - train_size\n",
        "train_dataset, test_dataset = th.utils.data.random_split(DataBase, [train_size, test_size])\n",
        "\n",
        "print(colored('The size of training set is ', 'yellow'), len(train_dataset))\n",
        "print(colored('The size of Test set is ', 'yellow'), len(test_dataset))\n",
        "\n",
        "# Dataloaders\n",
        "\n",
        "my_dataloader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "my_testloader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# DNN architecture parameters\n",
        "\n",
        "Networks_Main_Menu = Networks_activations(Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids,\n",
        "                                        n_input,\n",
        "                                        n_hidden,\n",
        "                                        n_output_reg,\n",
        "                                        n_output_clas,\n",
        "                                        p_dropout,\n",
        "                                        out_channel,\n",
        "                                        kernel_s,\n",
        "                                        padding)\n",
        "\n",
        "Model_m_task = Networks_Main_Menu.Network_m_Task()\n",
        "\n",
        "# DNN OPTIMIZER\n",
        "\n",
        "optimizer_m_task = th.optim.Adam(Model_m_task.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# scheduler lr\n",
        "\n",
        "scheduler_MT = ReduceLROnPlateau(optimizer_m_task, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Main training loop\n",
        "\n",
        "\n",
        "if BF_approach == 'HCL':\n",
        "    # initialing the loss function\n",
        "    criterium_clas_4d = Loss_HBF_Rate_Based_4D(Us, Mr, Nrf, Noise_pwr).to(device)\n",
        "\n",
        "    RATE_Predicted_HBF1= [] \n",
        "    #loss_clas1=[]\n",
        "\n",
        "\n",
        "    for i in range(1, epoch_size):\n",
        "      \n",
        "        for k, (channelR, channelI, alpha, RSSI, UR, UI, AR, AI, index, WR, WI, deltaR, deltaI, userp) in enumerate(my_dataloader):\n",
        "            \n",
        "            Inputs_Reg = Networks_Main_Menu.Inp_MT(RSSI)\n",
        "\n",
        "            # Loading the CSI (real and imaginary)\n",
        "            channelR = channelR.view(-1, Us, Mr).to(device)\n",
        "            channelI = channelI.view(-1, Us, Mr).to(device)\n",
        "\n",
        "            # Set gradient to 0.\n",
        "            optimizer_m_task.zero_grad()\n",
        "\n",
        "            # Feed forward multi-tasking DNN\n",
        "            Model_m_task.train()  #edited\n",
        "            out1_reg, out2_reg, out_clas = Model_m_task(Inputs_Reg)\n",
        "\n",
        "            # computing the loss fucntion for HBF using eq(25)\n",
        "            w_outr, w_outi = out1_reg.view(-1, Us, Nrf), out2_reg.view(-1, Us, Nrf)\n",
        "            HBF_all_4d = criterium_clas_4d(w_outr.permute(0, 2, 1), w_outi.permute(0, 2, 1), channelR, channelI,\n",
        "                th.unsqueeze(codesr.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device),\n",
        "                th.unsqueeze(codesi.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device))\n",
        "            loss_clas = FLP_loss(out_clas, HBF_all_4d)\n",
        "            #loss_clas1.append(loss_clas)\n",
        "\n",
        "            # Gradient calculation.\n",
        "            loss_clas.backward()\n",
        "\n",
        "            \n",
        "            # Model weight modification based on the optimizer.\n",
        "            optimizer_m_task.step()\n",
        "          \n",
        "            \n",
        "            # iterate through test dataset\n",
        "            if k == 0 or i % epoch_size == 0:\n",
        "                R_predicted_HBF = []\n",
        "                R_optimum_HBF = []\n",
        "\n",
        "\n",
        "                with th.no_grad():\n",
        "                    for (tchannelR, tchannelI, talpha, tRSSI, tUR, tUI, tAR, tAI, tindex, tWR, tWI, tdeltaR, tdeltaI, tup) in my_testloader:\n",
        "\n",
        "                   \n",
        "                        #e\n",
        "                        testInputs_Reg = Networks_Main_Menu.Inp_MT(tRSSI)\n",
        "\n",
        "                        # Loading the near-optimal digital precoder, CSI (real and imaginary)\n",
        "                        T_wR = tWR.reshape(-1, Us, Nrf).to(device)\n",
        "                        T_wI = tWI.reshape(-1, Us, Nrf).to(device)\n",
        "                        T_channelR = tchannelR.reshape(-1, Us, Mr).to(device)\n",
        "                        T_channelI = tchannelI.reshape(-1, Us, Mr).to(device)\n",
        "\n",
        "                        # Forward pass reg\n",
        "                        Model_m_task.eval()\n",
        "                        start_time = time.time()\n",
        "                        pred1_reg, pred2_reg, pred_class = Model_m_task(testInputs_Reg)\n",
        "\n",
        "                        # find the maximum probability as predication of classification\n",
        "                        _, predicted = th.max(F.softmax(pred_class, 1), 1)\n",
        "\n",
        "                        # mapping in the codebook to find the corresponding analog precoder\n",
        "                        An_Predr = codesr[predicted, :].to(device)\n",
        "                        An_Predi = codesi[predicted, :].to(device)\n",
        "                        w_prer, w_prei = pred1_reg.view(-1, Us, Nrf), pred2_reg.view(-1, Us, Nrf)\n",
        "\n",
        "                        # rate calculation\n",
        "                        # DNN HBF\n",
        "                        R_predicted_HBF.append(criterium_clas_4d.evaluate_rate(w_prer, w_prei, T_channelR, T_channelI, An_Predr, An_Predi))\n",
        "                        \n",
        "                        # near-optimal HBF\n",
        "                        R_optimum_HBF.append(criterium_clas_4d.evaluate_rate(T_wR, T_wI, T_channelR, T_channelI, tAR.to(device), tAI.to(device)))\n",
        "                        end_time = time.time()\n",
        "\n",
        "                        # Calculate inference time\n",
        "                        inference_time = end_time - start_time\n",
        "\n",
        "                # Average over all mini-batches\n",
        "                RATE_Predicted_HBF = sum(R_predicted_HBF) / len(R_predicted_HBF)\n",
        "                RATE_Optimum_HBF = sum(R_optimum_HBF) / len(R_optimum_HBF)\n",
        "                RATE_Ratie_HBF = 100 * RATE_Predicted_HBF / RATE_Optimum_HBF\n",
        "                RATE_Predicted_HBF1.append(RATE_Predicted_HBF)\n",
        "                scheduler_MT.step(RATE_Predicted_HBF)\n",
        "               \n",
        "                print('Iter:==>{:3d}  Loss_Class:{:.3f} Rate_opt_HBF:{:.2f} Rate_pre_HBF:{:.2f} Ratio_HBF:{:.2f}% Inference time:{:.2f}'.\n",
        "                    format(i, loss_clas, RATE_Optimum_HBF, RATE_Predicted_HBF,  RATE_Ratie_HBF, inference_time))\n",
        "                \n",
        "else:\n",
        "    raise Exception('BF_approach is wrong !!')"
      ],
      "metadata": {
        "id": "flaxHtZ-kBAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f95a7ca-6b6c-4cb3-edff-dabb3cb8435d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Cuda available?  False\n",
            "Which devide? cpu\n",
            "You select core dataset\n",
            "DataBase_dataSet64x8x4_130dB_0129201820.npy is loading ... \n",
            "The number of Unique AP in I1:  5\n",
            "The length of the codebook:  5\n",
            "The size of training set is  8568\n",
            "The size of Test set is  1512\n",
            "Iter:==>  1  Loss_Class:-11.747 Rate_opt_HBF:5.33 Rate_pre_HBF:1.44 Ratio_HBF:26.94% Inference time:0.36\n",
            "Iter:==>  2  Loss_Class:-251.944 Rate_opt_HBF:5.33 Rate_pre_HBF:2.60 Ratio_HBF:48.80% Inference time:0.36\n",
            "Iter:==>  3  Loss_Class:-478.484 Rate_opt_HBF:5.33 Rate_pre_HBF:3.06 Ratio_HBF:57.34% Inference time:0.36\n",
            "Iter:==>  4  Loss_Class:-667.726 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.87% Inference time:0.37\n",
            "Iter:==>  5  Loss_Class:-888.003 Rate_opt_HBF:5.33 Rate_pre_HBF:2.78 Ratio_HBF:52.23% Inference time:0.35\n",
            "Iter:==>  6  Loss_Class:-1098.886 Rate_opt_HBF:5.33 Rate_pre_HBF:2.88 Ratio_HBF:54.03% Inference time:0.39\n",
            "Iter:==>  7  Loss_Class:-1331.501 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.11% Inference time:0.47\n",
            "Iter:==>  8  Loss_Class:-1544.752 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.07% Inference time:0.49\n",
            "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Iter:==>  9  Loss_Class:-1813.429 Rate_opt_HBF:5.33 Rate_pre_HBF:2.77 Ratio_HBF:51.90% Inference time:0.34\n",
            "Iter:==> 10  Loss_Class:-1899.217 Rate_opt_HBF:5.33 Rate_pre_HBF:2.76 Ratio_HBF:51.73% Inference time:0.37\n",
            "Iter:==> 11  Loss_Class:-1911.791 Rate_opt_HBF:5.33 Rate_pre_HBF:2.77 Ratio_HBF:51.97% Inference time:0.36\n",
            "Iter:==> 12  Loss_Class:-1965.447 Rate_opt_HBF:5.33 Rate_pre_HBF:2.81 Ratio_HBF:52.67% Inference time:0.36\n",
            "Iter:==> 13  Loss_Class:-1972.795 Rate_opt_HBF:5.33 Rate_pre_HBF:2.81 Ratio_HBF:52.63% Inference time:0.36\n",
            "Iter:==> 14  Loss_Class:-2007.387 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.94% Inference time:0.36\n",
            "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Iter:==> 15  Loss_Class:-2046.183 Rate_opt_HBF:5.33 Rate_pre_HBF:2.81 Ratio_HBF:52.65% Inference time:0.50\n",
            "Iter:==> 16  Loss_Class:-2013.084 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.81% Inference time:0.49\n",
            "Iter:==> 17  Loss_Class:-2019.786 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.02% Inference time:0.49\n",
            "Iter:==> 18  Loss_Class:-2055.875 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.09% Inference time:0.49\n",
            "Iter:==> 19  Loss_Class:-2067.481 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.15% Inference time:0.49\n",
            "Iter:==> 20  Loss_Class:-2042.652 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.26% Inference time:0.49\n",
            "Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Iter:==> 21  Loss_Class:-2035.537 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.16% Inference time:0.49\n",
            "Iter:==> 22  Loss_Class:-2043.034 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.08% Inference time:0.35\n",
            "Iter:==> 23  Loss_Class:-2042.978 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.98% Inference time:0.36\n",
            "Iter:==> 24  Loss_Class:-2059.573 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.91% Inference time:0.36\n",
            "Iter:==> 25  Loss_Class:-2072.825 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.20% Inference time:0.37\n",
            "Iter:==> 26  Loss_Class:-2064.399 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.32% Inference time:0.41\n",
            "Epoch 00027: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Iter:==> 27  Loss_Class:-2037.956 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.11% Inference time:0.36\n",
            "Iter:==> 28  Loss_Class:-2051.298 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.05% Inference time:0.37\n",
            "Iter:==> 29  Loss_Class:-2050.542 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.19% Inference time:0.36\n",
            "Iter:==> 30  Loss_Class:-2074.906 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.24% Inference time:0.36\n",
            "Iter:==> 31  Loss_Class:-2049.144 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.09% Inference time:0.36\n",
            "Iter:==> 32  Loss_Class:-2062.218 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.93% Inference time:0.46\n",
            "Epoch 00033: reducing learning rate of group 0 to 1.0000e-08.\n",
            "Iter:==> 33  Loss_Class:-2057.348 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:52.99% Inference time:0.48\n",
            "Iter:==> 34  Loss_Class:-2053.685 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.04% Inference time:0.37\n",
            "Iter:==> 35  Loss_Class:-2073.830 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.22% Inference time:0.35\n",
            "Iter:==> 36  Loss_Class:-2044.279 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.38% Inference time:0.35\n",
            "Iter:==> 37  Loss_Class:-2073.879 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.26% Inference time:0.37\n",
            "Iter:==> 38  Loss_Class:-2043.339 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.42% Inference time:0.36\n",
            "Iter:==> 39  Loss_Class:-2061.035 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.24% Inference time:0.36\n",
            "Iter:==> 40  Loss_Class:-2043.915 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.28% Inference time:0.38\n",
            "Iter:==> 41  Loss_Class:-2044.161 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.15% Inference time:0.49\n",
            "Iter:==> 42  Loss_Class:-2080.762 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.16% Inference time:0.50\n",
            "Iter:==> 43  Loss_Class:-2045.409 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.31% Inference time:0.41\n",
            "Iter:==> 44  Loss_Class:-2042.182 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.18% Inference time:0.36\n",
            "Iter:==> 45  Loss_Class:-2070.420 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.29% Inference time:0.36\n",
            "Iter:==> 46  Loss_Class:-2067.172 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.30% Inference time:0.37\n",
            "Iter:==> 47  Loss_Class:-2039.814 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.28% Inference time:0.36\n",
            "Iter:==> 48  Loss_Class:-2048.991 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.06% Inference time:0.37\n",
            "Iter:==> 49  Loss_Class:-2056.245 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.88% Inference time:0.43\n",
            "Iter:==> 50  Loss_Class:-2036.311 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.83% Inference time:0.48\n",
            "Iter:==> 51  Loss_Class:-2041.934 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.01% Inference time:0.36\n",
            "Iter:==> 52  Loss_Class:-2063.018 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.15% Inference time:0.36\n",
            "Iter:==> 53  Loss_Class:-2037.199 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.24% Inference time:0.37\n",
            "Iter:==> 54  Loss_Class:-2055.466 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.15% Inference time:0.37\n",
            "Iter:==> 55  Loss_Class:-2091.404 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.42% Inference time:0.36\n",
            "Iter:==> 56  Loss_Class:-2046.051 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.12% Inference time:0.36\n",
            "Iter:==> 57  Loss_Class:-2024.794 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.28% Inference time:0.37\n",
            "Iter:==> 58  Loss_Class:-2031.740 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.27% Inference time:0.36\n",
            "Iter:==> 59  Loss_Class:-2064.551 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.31% Inference time:0.36\n",
            "Iter:==> 60  Loss_Class:-2048.190 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.04% Inference time:0.37\n",
            "Iter:==> 61  Loss_Class:-2048.472 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.00% Inference time:0.41\n",
            "Iter:==> 62  Loss_Class:-2066.546 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.36% Inference time:0.36\n",
            "Iter:==> 63  Loss_Class:-2056.977 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.32% Inference time:0.36\n",
            "Iter:==> 64  Loss_Class:-2046.340 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.35% Inference time:0.49\n",
            "Iter:==> 65  Loss_Class:-2047.984 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.20% Inference time:0.45\n",
            "Iter:==> 66  Loss_Class:-2058.172 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.11% Inference time:0.38\n",
            "Iter:==> 67  Loss_Class:-2066.049 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.13% Inference time:0.36\n",
            "Iter:==> 68  Loss_Class:-2055.992 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.15% Inference time:0.36\n",
            "Iter:==> 69  Loss_Class:-2067.944 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.29% Inference time:0.35\n",
            "Iter:==> 70  Loss_Class:-2047.776 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.30% Inference time:0.37\n",
            "Iter:==> 71  Loss_Class:-2062.858 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.13% Inference time:0.36\n",
            "Iter:==> 72  Loss_Class:-2037.348 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.91% Inference time:0.48\n",
            "Iter:==> 73  Loss_Class:-2070.662 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.91% Inference time:0.48\n",
            "Iter:==> 74  Loss_Class:-2074.129 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.23% Inference time:0.37\n",
            "Iter:==> 75  Loss_Class:-2057.384 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.38% Inference time:0.36\n",
            "Iter:==> 76  Loss_Class:-2042.479 Rate_opt_HBF:5.33 Rate_pre_HBF:2.85 Ratio_HBF:53.54% Inference time:0.37\n",
            "Iter:==> 77  Loss_Class:-2033.340 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.11% Inference time:0.37\n",
            "Iter:==> 78  Loss_Class:-2050.677 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.01% Inference time:0.49\n",
            "Iter:==> 79  Loss_Class:-2049.940 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.10% Inference time:0.47\n",
            "Iter:==> 80  Loss_Class:-2055.874 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.09% Inference time:0.37\n",
            "Iter:==> 81  Loss_Class:-2014.714 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.10% Inference time:0.36\n",
            "Iter:==> 82  Loss_Class:-2062.933 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.03% Inference time:0.36\n",
            "Iter:==> 83  Loss_Class:-2071.205 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.26% Inference time:0.36\n",
            "Iter:==> 84  Loss_Class:-2065.631 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.19% Inference time:0.35\n",
            "Iter:==> 85  Loss_Class:-2062.599 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.19% Inference time:0.37\n",
            "Iter:==> 86  Loss_Class:-2034.002 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.12% Inference time:0.51\n",
            "Iter:==> 87  Loss_Class:-2043.792 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.23% Inference time:0.49\n",
            "Iter:==> 88  Loss_Class:-2075.927 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.17% Inference time:0.49\n",
            "Iter:==> 89  Loss_Class:-2021.684 Rate_opt_HBF:5.33 Rate_pre_HBF:2.82 Ratio_HBF:52.97% Inference time:0.49\n",
            "Iter:==> 90  Loss_Class:-2057.048 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.04% Inference time:0.37\n",
            "Iter:==> 91  Loss_Class:-2039.209 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.14% Inference time:0.37\n",
            "Iter:==> 92  Loss_Class:-2039.181 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.26% Inference time:0.37\n",
            "Iter:==> 93  Loss_Class:-2057.439 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.27% Inference time:0.38\n",
            "Iter:==> 94  Loss_Class:-2042.763 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.17% Inference time:0.36\n",
            "Iter:==> 95  Loss_Class:-2043.283 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.31% Inference time:0.37\n",
            "Iter:==> 96  Loss_Class:-2042.363 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.21% Inference time:0.37\n",
            "Iter:==> 97  Loss_Class:-2053.543 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.10% Inference time:0.37\n",
            "Iter:==> 98  Loss_Class:-2050.738 Rate_opt_HBF:5.33 Rate_pre_HBF:2.84 Ratio_HBF:53.28% Inference time:0.36\n",
            "Iter:==> 99  Loss_Class:-2028.220 Rate_opt_HBF:5.33 Rate_pre_HBF:2.83 Ratio_HBF:53.04% Inference time:0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.save(\"/content/drive/MyDrive/HBF-Net-main/final/hclloss/gumblesig.npy\", RATE_Predicted_HBF1)"
      ],
      "metadata": {
        "id": "o8m2UfpFsTsG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss_clas1\n",
        "losses= [ loss.detach().numpy() for loss in loss_clas1]\n",
        "ab=np.array(losses)\n",
        "np.save(\"/content/drive/MyDrive/HBF-Net-main/final/hclloss/fdp32loss.npy\", ab)"
      ],
      "metadata": {
        "id": "3ggcf7BesUtn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}