{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umegyp4BjlHA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "\n",
        "def Th_comp_matmul(Ar, Ai, Br, Bi):  # Complex matmul pytorch function ########\n",
        "    if Ar.ndim == 3 and Br.ndim == 3:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, 0:int(c_th.shape[1] / 2), 0:int(c_th.shape[2] / 2)]\n",
        "        c_th_i = c_th[:, int(c_th.shape[1] / 2):, 0:int(c_th.shape[2] / 2)]\n",
        "    elif Ar.ndim == 2 and Br.ndim == 2:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=1), torch.cat((Ai, Ar), dim=1)), dim=0)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=1), torch.cat((Bi, Br), dim=1)), dim=0)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[0:int(c_th.shape[0] / 2), 0:int(c_th.shape[1] / 2)]\n",
        "        c_th_i = c_th[int(c_th.shape[0] / 2):, 0:int(c_th.shape[1] / 2)]\n",
        "    elif Ar.ndim == 4 and Br.ndim == 4:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "        c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim == 5 and Br.ndim == 5:\n",
        "        a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "        b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "        c_th = torch.matmul(a_th, b_th)\n",
        "        c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "        c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 12:\n",
        "        if Ar.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=2), torch.cat((Bi, Br), dim=2)), dim=1)\n",
        "          \n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "        elif Br.ndim == 4:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=2), torch.cat((Ai, Ar), dim=2)), dim=1)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, 0:int(c_th.shape[2] / 2), 0:int(c_th.shape[3] / 2)]\n",
        "            c_th_i = c_th[:, :, int(c_th.shape[2] / 2):, 0:int(c_th.shape[3] / 2)]\n",
        "    elif Ar.ndim * Br.ndim == 20:\n",
        "        if Ar.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=4), torch.cat((Ai, Ar), dim=4)), dim=3)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=3), torch.cat((Bi, Br), dim=3)), dim=2)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "        elif Br.ndim == 5:\n",
        "            a_th = torch.cat((torch.cat((Ar, -Ai), dim=3), torch.cat((Ai, Ar), dim=3)), dim=2)\n",
        "            b_th = torch.cat((torch.cat((Br, -Bi), dim=4), torch.cat((Bi, Br), dim=4)), dim=3)\n",
        "            c_th = torch.matmul(a_th, b_th)\n",
        "            c_th_r = c_th[:, :, :, 0:int(c_th.shape[3] / 2), 0:int(c_th.shape[4] / 2)]\n",
        "            c_th_i = c_th[:, :, :, int(c_th.shape[3] / 2):, 0:int(c_th.shape[4] / 2)]\n",
        "    else:\n",
        "        raise Exception('the dimension is not defined for Th_comp_matmul.')\n",
        "\n",
        "    return c_th_r, c_th_i\n",
        "\n",
        "def Th_inv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    Ar_inv = torch.inverse(Ar + torch.matmul(torch.matmul(Ai, torch.inverse(Ar)), Ai))\n",
        "    Ai_inv = - torch.matmul(torch.matmul(torch.inverse(Ar), Ai), Ar_inv)\n",
        "    return Ar_inv, Ai_inv\n",
        "\n",
        "def Th_pinv(Ar, Ai):  # Complex inverse pytorch function ########\n",
        "    if Ar.ndim == 2:\n",
        "        if Ar.shape[0] < Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.T, -Ai.T)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.T, -Ai.T, Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[0] > Ar.shape[1]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.T, -Ai.T, Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.T, -Ai.T)\n",
        "        elif Ar.shape[0] == Ar.shape[1]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 3:\n",
        "        if Ar.shape[1] < Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[1] > Ar.shape[2]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 2, 1), -Ai.permute(0, 2, 1))\n",
        "        elif Ar.shape[1] == Ar.shape[2]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    elif Ar.ndim == 4:\n",
        "        if Ar.shape[2] < Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar, Ai, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar_inv, Ai_inv)\n",
        "        elif Ar.shape[2] > Ar.shape[3]:\n",
        "            Tempr, Tempi = Th_comp_matmul(Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2), Ar, Ai)\n",
        "            Ar_inv, Ai_inv = Th_inv(Tempr, Tempi)\n",
        "            return Th_comp_matmul(Ar_inv, Ai_inv, Ar.permute(0, 1, 3, 2), -Ai.permute(0, 1, 3, 2))\n",
        "        elif Ar.shape[2] == Ar.shape[3]:\n",
        "            return Th_inv(Ar, Ai)\n",
        "    else:\n",
        "        raise Exception('5-D is not defined for Th_pinv.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ssl import CHANNEL_BINDING_TYPES\n",
        "import torch.utils.data as data\n",
        "from termcolor import colored\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.decomposition import FastICA\n",
        "# Database ####################################################################################################################\n",
        "class Data_Reader(data.Dataset):\n",
        "    def __init__(self, filename, Us, Mr, Nrf, K):\n",
        "\n",
        "        print(colored('You select core dataset', 'cyan'))\n",
        "        print(colored(filename, 'yellow'), 'is loading ... ')\n",
        "        np_data = np.load(filename)\n",
        "        #file3=np.load('/content/drive/MyDrive/HBF-Net-main/dataset/DataBase_pro_channel_BS32_2p4GHz_1Path.npy')\n",
        "        transformer = IncrementalPCA(n_components=128, batch_size=500)     \n",
        "        RSSI_N1 = np_data[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        #RSSI_N1 = file3[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        X_sparse = sparse.csr_matrix(RSSI_N1)\n",
        "        np_data2 = transformer.fit_transform(X_sparse)\n",
        "\n",
        "\n",
        "        #pca = PCA(n_components=128, svd_solver='full')\n",
        "        #pca = PCA(n_components=1, svd_solver='arpack')\n",
        "        #transformer = SparsePCA(n_components=128, random_state=0)\n",
        "        #transformer = KernelPCA(n_components=128, kernel='linear')\n",
        "        #np_data2 = transformer.fit_transform(RSSI_N1)\n",
        "        #np_data2= pca.fit_transform(RSSI_N1)\n",
        "        #model = NMF(n_components=128, init='random', random_state=0)\n",
        "        #np_data2 = model.fit_transform(RSSI_N1)\n",
        "        #transformer = FactorAnalysis(n_components=128, random_state=0)\n",
        "        #X_transformed = transformer.fit_transform(RSSI_N1)\n",
        "        #transformer = FastICA(n_components=128,\n",
        "                              #random_state=0,\n",
        "                              #whiten='unit-variance')\n",
        "        #np_data2 = transformer.fit_transform(RSSI_N1)\n",
        "        #model = NMF(n_components=128, init='random', random_state=0)\n",
        "        #np_data3 = model.fit_transform(model)       \n",
        "        #transformer = KernelPCA(n_components=128, kernel='linear')\n",
        "        #np_data3 = transformer.fit_transform(np_data2)\n",
        "\n",
        "\n",
        "        self.channelR = np_data[:, 0:Us * Mr].real.astype(float)\n",
        " \n",
        "        self.channelI = np_data[:, 0:Us * Mr].imag.astype(float)\n",
        "\n",
        "        self.alpha = np_data[:, Us * Mr: (Us * Mr) + (Us * K)].real.astype(float)\n",
        "\n",
        "        #self.RSSI_N = np_data[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        self.RSSI_N = np_data2\n",
        "        #self.RSSI_N1 = np_data2[:, (Us * Mr) + (Us * K):(Us * Mr) + (2 * Us * K)].real.astype(float)\n",
        "        self.UR = np_data[:, (Us * Mr) + (2 * Us * K):(2 * Us * Mr) + \n",
        "                          (2 * Us * K)].real.astype(float)\n",
        "        self.UI = np_data[:, (Us * Mr) + (2 * Us * K):(2 * Us * Mr) + \n",
        "                          (2 * Us * K)].imag.astype(float)\n",
        "\n",
        "        self.AR = np_data[:, Us * (2 * Mr + 2 * K):Us * (2 * Mr + 2 * K) + \n",
        "                          (Nrf * Mr)].real.astype(float)\n",
        "        self.AI = np_data[:, Us * (2 * Mr + 2 * K):Us * (2 * Mr + 2 * K) + \n",
        "                          (Nrf * Mr)].imag.astype(float)\n",
        "\n",
        "        self.target = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr):Us * (2 * Mr + 2 * K) + \n",
        "                              (Nrf * Mr) + 1].real.astype(int)\n",
        "\n",
        "        self.WR = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr) + 1:Us * \n",
        "                          (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1].real.astype(float)\n",
        "        self.WI = np_data[:, Us * (2 * Mr + 2 * K) + (Nrf * Mr) + 1:Us * \n",
        "                          (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1].imag.astype(float)\n",
        "\n",
        "        self.deltaR = np_data[:, Us * (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1:Us * \n",
        "                              (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1].real.astype(float)\n",
        "        self.deltaI = np_data[:, Us * (2 * Mr + 2 * K + Nrf) + (Nrf * Mr) + 1:Us * \n",
        "                              (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1].imag.astype(float)\n",
        "\n",
        "        self.userp = np_data[:, Us * (2 * Mr + 3 * K + Nrf) + (Nrf * Mr) + 1: Us * \n",
        "                             (2 * Mr + 3 * K + Nrf + 2) + (Nrf * Mr) + 1].real.astype(float)\n",
        "\n",
        "        self.n_samples = np_data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def uniq_clas(self):\n",
        "        uniq = np.unique(self.target, return_counts=True)\n",
        "        NO_Class = np.unique(self.target).shape[0]\n",
        "        print(colored(\"The number of Unique AP in I1: \", \"green\"), NO_Class)\n",
        "        return np.max(uniq[1]) * 100 / uniq[1].sum()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(self.channelR[index]), torch.Tensor(self.channelI[index]), torch.Tensor(self.alpha[index]), \\\n",
        "            torch.Tensor(self.RSSI_N[index]), torch.Tensor(self.UR[index]), torch.Tensor(self.UI[index]), torch.Tensor(self.AR[index]), \\\n",
        "                torch.Tensor(self.AI[index]), torch.LongTensor(self.target[index]), torch.Tensor(self.WR[index]), torch.Tensor(self.WI[index]), \\\n",
        "                    torch.Tensor(self.deltaR[index]), torch.Tensor(self.deltaI[index]), torch.Tensor(self.userp[index])\n",
        "\n",
        "# readme reader for HBF initial parameters ####################################################################################\n",
        "def md_reader(DB_name):\n",
        "    md = genfromtxt('DATASET.md', delimiter='\\n', dtype='str')\n",
        "    Us = int(re.findall(r'\\d+', md[1])[0])\n",
        "    Mr = int(re.findall(r'\\d+', md[2])[0])\n",
        "    Nrf = int(re.findall(r'\\d+', md[3])[0])\n",
        "    Ass_n = int(re.findall(r'\\d+', md[4])[0])\n",
        "    Noise_pwr = float(''.join(('1e-', str(int(int(re.findall(r'\\d+', md[6])[0]) / 10)))))\n",
        "    return Us, Mr, Nrf, Ass_n, Noise_pwr\n",
        "\n",
        "class Initialization_Model_Params(object):\n",
        "    def __init__(self,\n",
        "                 DB_name,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids\n",
        "                 ):\n",
        "        self.DB_name = DB_name\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "\n",
        "    def Data_Load(self):\n",
        "        DataBase = Data_Reader(''.join(('DataBase_', self.DB_name, '.npy')),\n",
        "                               self.Us, self.Mr, self.Nrf, self.K)\n",
        "        uniq_dis_label = DataBase.uniq_clas()\n",
        "        return DataBase, uniq_dis_label\n",
        "\n",
        "    def Code_Read(self):\n",
        "        codes = genfromtxt('/content/drive/MyDrive/HBF-Net-main/Codebook/Codebook_ij.csv', delimiter=',', dtype='complex', skip_header=0)\n",
        "        #codes = genfromtxt('/content/drive/MyDrive/HBF-Net-main/Codebook/newexample2.csv', delimiter=',', dtype='complex', skip_header=0)\n",
        "        label = np.arange(len(codes))\n",
        "        self.n_output_clas = len(codes)\n",
        "        print(colored(\"The length of the codebook: \", \"green\"), len(codes))\n",
        "        Codes_idx = np.concatenate((label[:, np.newaxis], codes), axis=1)\n",
        "        codeword_C = {}\n",
        "        index_C = []\n",
        "        for i in range(len(codes)):\n",
        "            index_C = Codes_idx[i, 0].real.astype(int)\n",
        "            icode_C = Codes_idx[i, 1:]\n",
        "            codeword_C[index_C] = icode_C\n",
        "\n",
        "        # torch tensor of codes\n",
        "        codesr = torch.from_numpy(codes.real).type(torch.float)\n",
        "        codesi = torch.from_numpy(codes.imag).type(torch.float)\n",
        "        return codeword_C, len(codes), codesr, codesi\n",
        "\n",
        "class Loss_FDP_Rate_Based(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_FDP_Rate_Based, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=1, dim2=2)\n",
        "\n",
        "        SINR = diag_W / (torch.sum(W, 2) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(1)\n",
        "\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return -sum_rate.mean()\n",
        "\n",
        "    def evaluate_rate(self, outr, outi, channelr, channeli):\n",
        "        outr = outr.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "        outi = outi.view(-1, self.Us, self.Mr).permute(0, 2, 1)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(outr.flatten(1) ** 2 + outi.flatten(1) ** 2, dim=1))\n",
        "        outr = (outr.flatten(1) / temp_pre.unsqueeze(1)).view(outr.shape)\n",
        "        outi = (outi.flatten(1) / temp_pre.unsqueeze(1)).view(outi.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, outr, outi, channelr, channeli)\n",
        "        return sum_rate.mean()\n",
        "\n",
        "class Loss_HBF_Rate_Based_4D(torch.nn.Module):\n",
        "    def __init__(self, Us, Mr, Nrf, Noise_pwr):\n",
        "        super(Loss_HBF_Rate_Based_4D, self).__init__()\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.noise_power = Noise_pwr\n",
        "\n",
        "    def rate_calculator_4d(self, u_re, u_im, channelr, channeli):\n",
        "        Wr, Wi = Th_comp_matmul(channelr, -channeli, u_re, u_im)\n",
        "        W = Wr**2 + Wi**2\n",
        "        diag_W = torch.diagonal(W, dim1=2, dim2=3)\n",
        "        SINR = diag_W / (torch.sum(W, 3) - diag_W + self.noise_power)\n",
        "        userRates = torch.log2(1 + SINR)\n",
        "        sumRate = userRates.sum(2)\n",
        "        return sumRate\n",
        "\n",
        "    def forward(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2),\n",
        "                                            Ai.view(-1, len(channelr), self.Nrf, self.Mr).permute(0, 1, 3, 2), Wr, Wi)\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(2) ** 2 + HBF_prei.flatten(2) ** 2, dim=2))\n",
        "        HBF_prer = (HBF_prer.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(2) / temp_pre.unsqueeze(2)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_HBF_Rate_Based_4D.rate_calculator_4d(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "        return sum_rate.T\n",
        "\n",
        "    def evaluate_rate(self, Wr, Wi, channelr, channeli, Ar, Ai):\n",
        "        HBF_prer, HBF_prei = Th_comp_matmul(Ar.view(-1, self.Nrf, self.Mr).permute(0, 2, 1),\n",
        "            Ai.view(-1, self.Nrf, self.Mr).permute(0, 2, 1), Wr.permute(0, 2, 1), Wi.permute(0, 2, 1))\n",
        "\n",
        "        # power normalization over all antennas\n",
        "        temp_pre = torch.sqrt(torch.sum(HBF_prer.flatten(1) ** 2 + HBF_prei.flatten(1) ** 2, dim=1))\n",
        "        HBF_prer = (HBF_prer.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prer.shape)\n",
        "        HBF_prei = (HBF_prei.flatten(1) / temp_pre.unsqueeze(1)).view(HBF_prei.shape)\n",
        "\n",
        "        sum_rate = Loss_FDP_Rate_Based.rate_calculator(self, HBF_prer, HBF_prei, channelr, channeli)\n",
        "\n",
        "        return sum_rate.mean()\n",
        "\n",
        "def FLP_loss(x, y):\n",
        "    log_prob = - 1.0 * F.softmax(x, 1)\n",
        "    temp = log_prob * y\n",
        "    cel = temp.sum(dim=1)\n",
        "    cel = cel.mean()\n",
        "    return cel\n"
      ],
      "metadata": {
        "id": "MrjpZv0djsuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Net_m_task_CNN(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden, n_out_Reg, n_out_clas, p_dropout, U, Ass, out_channel, kernel_s, padding):\n",
        "        super(Net_m_task_CNN, self).__init__()\n",
        "\n",
        "        #self.cnn1 = ComplexConv2D(4,4,in_channels=1, out_channels=16, stride=1, padding=padding)\n",
        "        \n",
        "\n",
        "        #self.cnn1 = (ComplexConv2D(4, 4, strides=1, padding='valid',\n",
        "                          #kernel_initializer='complex_independent',name='Conv_P'))(encoded)\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do1 = nn.Dropout2d(p_dropout)\n",
        "        #self.do1 = nn.AlphaDropout(p_dropout)\n",
        "        #self.do1 = nn.FeatureAlphaDropout(0.2)\n",
        "\n",
        "\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "        self.do2 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do3 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "        self.cnn4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu4 = nn.LeakyReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=16)\n",
        "        self.do4 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "\n",
        "        self.cnn5 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=kernel_s, stride=1, padding=padding)\n",
        "        self.relu5 = nn.LeakyReLU()\n",
        "        self.bn5 = nn.BatchNorm2d(num_features=8)\n",
        "        self.do5 = nn.Dropout2d(p_dropout)\n",
        "\n",
        "       \n",
        "        \n",
        "        #n_in1 = 8\n",
        "        #self.word_embeddings = nn.Embedding.from_pretrained(0.5, freeze=True)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.do6 = nn.Dropout(0.05)\n",
        "        \n",
        "        self.rnn1 = nn.RNN(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.drnn1 = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.rnn2 = nn.RNN(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.drnn2 = nn.Dropout(0.05)\n",
        "\n",
        "        self.gru1 = nn.GRU(1024, 512,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.dgru1 = nn.Dropout(0.05)\n",
        "\n",
        "        self.gru2 = nn.GRU(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.dgru2 = nn.Dropout(0.05)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(512, 256,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.do7 = nn.Dropout(0.05)\n",
        "\n",
        "        #self.lstm3 = nn.LSTM(32, n_in,\n",
        "                            #bidirectional=True, batch_first=True)\n",
        "        \n",
        "        #self.do6 = nn.Dropout(p_dropout)\n",
        "#end\n",
        "\n",
        "\n",
        "        # Fully connected 1 (readout)\n",
        "        x_new = (U + 2 * padding - kernel_s) + 1\n",
        "        y_new = (Ass + 2 * padding - kernel_s) + 1\n",
        "\n",
        "        nn_in_fc = 8 * (x_new + 2 * padding - kernel_s + 1) * (y_new + 2 * padding - kernel_s + 1)\n",
        "\n",
        "\n",
        "        self.fc20 = nn.Linear(nn_in_fc, n_hidden)\n",
        "        self.bn20 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu20 = nn.LeakyReLU()\n",
        "        self.do20 = nn.Dropout(p_dropout)\n",
        "\n",
        "        self.fc30 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.bn30 = nn.BatchNorm1d(n_hidden)\n",
        "        self.relu30 = nn.LeakyReLU()\n",
        "        self.do30 = nn.Dropout(p_dropout)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc7R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc8R = nn.Linear(n_hidden, n_out_Reg)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc9C = nn.Linear(n_hidden, n_out_clas)\n",
        "\n",
        "    def forward(self, x):  # always\n",
        "\n",
        "        out = self.cnn1(x)\n",
        "        out = self.do1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "       # out = self.cnn2(out)\n",
        "        #out = self.do2(out)\n",
        "       # out = self.bn2(out)\n",
        "        #out = self.relu2(out)\n",
        "\n",
        "        ##out = self.cnn3(out)\n",
        "        #out = self.do3(out)\n",
        "        ##out = self.bn3(out)\n",
        "        #out = self.relu3(out)\n",
        "\n",
        "        out = self.cnn4(out)\n",
        "        out = self.do4(out)\n",
        "        out = self.bn4(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.cnn5(out)\n",
        "        out = self.do5(out)\n",
        "        out = self.bn5(out)\n",
        "        out = self.relu5(out)\n",
        "\n",
        "        #m6\n",
        "        #out= torch.flatten(out)\n",
        "        #out= out[0:131072]\n",
        "        #out = out.view(-1,1024)\n",
        "        out = out.view(out.size(0), -1)\n",
        "       # out = out.view(-1,64)\n",
        "        out = self.lstm1(out)\n",
        "        out = self.do6(out[0])\n",
        "        #out = self.gru1(out)\n",
        "        #out = self.dgru1(out[0]) \n",
        "\n",
        "        #out = self.rnn1(out)\n",
        "        #out = self.drnn1(out[0])\n",
        "        #out = self.lstm1(out)\n",
        "        #out = self.do4(out[0])\n",
        "        #out = out.view(-1,512)\n",
        "       # out = self.rnn2(out)\n",
        "        #out = self.drnn2(out[0])        \n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "       # out = self.lstm2(out)\n",
        "        #out = self.do7(out[0])  \n",
        "        #out = out.view(-1,512)\n",
        "\n",
        "       # out = self.gru1(out)\n",
        "        #out = self.dgru1(out[0]) \n",
        "       # out = self.lstm1(out)\n",
        "       # out = self.do4(out[0])\n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "        #out = self.gru2(out)\n",
        "        #out = self.dgru2(out[0]) \n",
        "        #out = out.view(-1,1024)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.fc20(out)\n",
        "        out = self.do20(out)\n",
        "        out = self.relu20(out)\n",
        "        out = self.bn20(out)\n",
        "\n",
        "        out = self.fc30(out)\n",
        "        out = self.do30(out)\n",
        "        out = self.relu30(out)\n",
        "        out = self.bn30(out)\n",
        "        #out= torch.flatten(out)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "        #out= out[1:131073]\n",
        "        #out = out.view(-1, 1024)\n",
        "        #out = self.rnn1(out)\n",
        "        #out = self.drnn1(out[0])\n",
        "        #out = out.view(-1,512)\n",
        "       # out = out.view(out.size(0), -1)\n",
        "        #out = self.rnn2(out)\n",
        "        #out = self.drnn2(out[0])        \n",
        "\n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,512)\n",
        "        #out = self.lstm2(out)\n",
        "        #out = self.do5(out[0]) \n",
        "        #out = self.gru2(out)\n",
        "        #out = self.dgru2(out[0])  \n",
        "\n",
        "        #out = self.lstm3(out)\n",
        "        #out = self.do6(out)     \n",
        "        #out = torch.flatten(out)\n",
        "        #out = out.view(-1,1024)\n",
        "        #out = self.lstm1(out)\n",
        "        #out = self.do6(out[0])\n",
        "       # out = out.view(out.size(0), -1)\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outR1 = self.fc7R(out)\n",
        "\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        #outR2 = self.fc8R(out)\n",
        "        outR2 = self.fc8R(out)\n",
        "        # Linear function (readout)  ****** LINEAR ******\n",
        "        outC = self.fc9C(out)\n",
        "\n",
        "        return outR1 ,outR2, outC"
      ],
      "metadata": {
        "id": "_k39ZKCXjuWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import RegressorMixin\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "\n",
        "\n",
        "class Networks_activations(object):\n",
        "    def __init__(self,\n",
        "                 Us,\n",
        "                 Mr,\n",
        "                 Nrf,\n",
        "                 K,\n",
        "                 K_limited,\n",
        "                 Noise_pwr,\n",
        "                 device,\n",
        "                 device_ids,\n",
        "                 n_input,\n",
        "                 n_hidden,\n",
        "                 n_output_reg,\n",
        "                 n_output_clas,\n",
        "                 p_dropout,\n",
        "                 out_channel,\n",
        "                 kernel_s,\n",
        "                 padding\n",
        "                 ):\n",
        "        self.Us = Us\n",
        "        self.Mr = Mr\n",
        "        self.Nrf = Nrf\n",
        "        self.K = K\n",
        "        self.K_limited = K_limited\n",
        "        self.Noise_pwr = Noise_pwr\n",
        "        self.device = device\n",
        "        self.dev_id = device_ids\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output_reg = n_output_reg\n",
        "        self.n_output_clas = n_output_clas\n",
        "        self.p_dropout = p_dropout\n",
        "        self.out_channel = out_channel\n",
        "        self.kernel_s = kernel_s\n",
        "        self.padding = padding\n",
        "        \n",
        "    def Network_m_Task(self):\n",
        "        if self.device.type == 'cuda':\n",
        "            return nn.DataParallel(Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas, self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding), device_ids=self.dev_id).to(self.device)\n",
        "        else:\n",
        "            return Net_m_task_CNN(self.n_input, self.n_hidden, self.n_output_reg, self.n_output_clas,\n",
        "                self.p_dropout, self.Us, self.K_limited, self.out_channel, self.kernel_s, self.padding)\n",
        "\n",
        "    def Inp_MT(self, RSSI):\n",
        "        #transformer = IncrementalPCA(n_components=12, batch_size=10080)\n",
        "        #in12 = torch.flatten(RSSI)\n",
        "        #Inputs_MT = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)  \n",
        "        #Inputs_MT1 = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)\n",
        "       # Inputs_MT2 = np.array(RSSI)\n",
        "        #X_sparse = sparse.csr_matrix(Inputs_MT2)\n",
        "\n",
        "        #Inputs_MT = transformer.fit_transform(X_sparse)\n",
        "       # RSSI =torch.Tensor(Inputs_MT)\n",
        "        #RSSI2 = torch.flatten(RSSI1)\n",
        "        #RSSI= RSSI2[:5000]\n",
        "        #Inputs_MT = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)        \n",
        "        Inputs_MT = RSSI.reshape(len(RSSI), 1, self.Us, self.K)[:, :, :, 0:self.K_limited].float().to(self.device)\n",
        "        return Inputs_MT"
      ],
      "metadata": {
        "id": "b6_mt2W1j7Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy import genfromtxt\n",
        "import torch as th\n",
        "import time\n",
        "from termcolor import colored\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "###############################################################################\n",
        "# Directory file\n",
        "###############################################################################\n",
        "DB_name = 'dataSet64x8x4_130dB_0129201820'\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Processor selection GPU if available (using GPU is highly recommended)\n",
        "###############################################################################\n",
        "device = th.device(\"cuda:2\" if th.cuda.is_available() else \"cpu\")\n",
        "device_ids = [2, 1, 3]\n",
        "print(\"Is Cuda available? \", colored('True', 'green')\n",
        "    if th.cuda.is_available() else colored('False', 'red'))\n",
        "print(\"Which devide?\", colored(device, 'cyan'))\n",
        "\n",
        "###############################################################################\n",
        "# Setup Parameters\n",
        "###############################################################################\n",
        "\n",
        "# Beamforming approach  AFP_Net, HBF_NET   ####################################\n",
        "BF_approach = 'HBF_Net'\n",
        "#BF_approach = 'AFP_Net'\n",
        "###############################################################################\n",
        "# Beamfroming system model and DNN Parameters\n",
        "###############################################################################\n",
        "#os.chdir(os.path.dirname(os.path.abspath('C:/Users/user/Desktop/dataSet64x8x4_130dB_0129201820.npy')))\n",
        "#os.chdir(os.path.dirname(os.path.abspath('/Desktop/DataBase_dataSet64x8x4_130dB_0129201820.npy')))\n",
        "os.chdir(os.path.dirname(os.path.abspath('/content/drive/MyDrive/HBF-Net-main/dataSet64x8x4_130dB_0129201820.npy')))\n",
        "Us, Mr, Nrf, K, Noise_pwr = md_reader(DB_name)                # Number of users, antenna, K, RF chains and noise power\n",
        "#m6\n",
        "#Us=8\n",
        "#Mr=128\n",
        "#Noise_pwr =120\n",
        "#K=\n",
        "#end\n",
        "\n",
        "\n",
        "K_limited = K                                                 # Number of SS as RSSI\n",
        "batch_size = 500                                          # Batch size\n",
        "epoch_size = 100                                         # Number of training epoches\n",
        "#lr = 0.001 \n",
        "lr = 0.001                                                # Learning rate\n",
        "#wd = 1e-6 \n",
        "wd = 1e-9                                                      # Weight decay\n",
        "n_input = Us * K_limited                                      # Input dimensions\n",
        "n_hidden = 1024                                         # Size of FCL layers\n",
        "out_channel = 16                                              # Size of CL channels\n",
        "kernel_s = 3                                                  # Size of Kernels in CL\n",
        "padding = 1                                                   # Size of padding in CL\n",
        "p_dropout = 0.05                                              # Probability of dropout\n",
        "\n",
        "\n",
        "\n",
        "if BF_approach == 'HCL':\n",
        "    n_output_reg = Us * Nrf\n",
        "\n",
        "else:\n",
        "    raise Exception('BF_approach value is wrong !!')\n",
        "\n",
        "Main_Menu = Initialization_Model_Params(DB_name,\n",
        "                                        Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids)\n",
        "\n",
        "\n",
        "# Reading Database\n",
        "\n",
        "DataBase, uniq_dis_label = Main_Menu.Data_Load()\n",
        "\n",
        "# Codeword dictionary\n",
        "\n",
        "#codeword_C, n_output_clas, codesr, codesi = Main_Menu.Code_Read()\n",
        "codeword_C, n_output_clas, codesr, codesi = Main_Menu.Code_Read()\n",
        "\n",
        "# Training-set and test-set generation\n",
        "\n",
        "train_size = int(0.85 * len(DataBase))\n",
        "test_size = len(DataBase) - train_size\n",
        "train_dataset, test_dataset = th.utils.data.random_split(DataBase, [train_size, test_size])\n",
        "\n",
        "print(colored('The size of training set is ', 'yellow'), len(train_dataset))\n",
        "print(colored('The size of Test set is ', 'yellow'), len(test_dataset))\n",
        "\n",
        "# Dataloaders\n",
        "\n",
        "my_dataloader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "my_testloader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# DNN architecture parameters\n",
        "\n",
        "Networks_Main_Menu = Networks_activations(Us,\n",
        "                                        Mr,\n",
        "                                        Nrf,\n",
        "                                        K,\n",
        "                                        K_limited,\n",
        "                                        Noise_pwr,\n",
        "                                        device,\n",
        "                                        device_ids,\n",
        "                                        n_input,\n",
        "                                        n_hidden,\n",
        "                                        n_output_reg,\n",
        "                                        n_output_clas,\n",
        "                                        p_dropout,\n",
        "                                        out_channel,\n",
        "                                        kernel_s,\n",
        "                                        padding)\n",
        "\n",
        "Model_m_task = Networks_Main_Menu.Network_m_Task()\n",
        "\n",
        "# DNN OPTIMIZER\n",
        "\n",
        "optimizer_m_task = th.optim.Adam(Model_m_task.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# scheduler lr\n",
        "\n",
        "scheduler_MT = ReduceLROnPlateau(optimizer_m_task, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Main training loop\n",
        "\n",
        "\n",
        "if BF_approach == 'HCL':\n",
        "    # initialing the loss function\n",
        "    criterium_clas_4d = Loss_HBF_Rate_Based_4D(Us, Mr, Nrf, Noise_pwr).to(device)\n",
        "\n",
        "    RATE_Predicted_HBF1= [] \n",
        "\n",
        "\n",
        "    for i in range(1, epoch_size):\n",
        "      \n",
        "        for k, (channelR, channelI, alpha, RSSI, UR, UI, AR, AI, index, WR, WI, deltaR, deltaI, userp) in enumerate(my_dataloader):\n",
        "            \n",
        "            Inputs_Reg = Networks_Main_Menu.Inp_MT(RSSI)\n",
        "\n",
        "            # Loading the CSI (real and imaginary)\n",
        "            channelR = channelR.view(-1, Us, Mr).to(device)\n",
        "            channelI = channelI.view(-1, Us, Mr).to(device)\n",
        "\n",
        "            # Set gradient to 0.\n",
        "            optimizer_m_task.zero_grad()\n",
        "\n",
        "            # Feed forward multi-tasking DNN\n",
        "            Model_m_task.train()  #edited\n",
        "            out1_reg, out2_reg, out_clas = Model_m_task(Inputs_Reg)\n",
        "\n",
        "            # computing the loss fucntion for HBF using eq(25)\n",
        "            w_outr, w_outi = out1_reg.view(-1, Us, Nrf), out2_reg.view(-1, Us, Nrf)\n",
        "            HBF_all_4d = criterium_clas_4d(w_outr.permute(0, 2, 1), w_outi.permute(0, 2, 1), channelR, channelI,\n",
        "                th.unsqueeze(codesr.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device),\n",
        "                th.unsqueeze(codesi.unsqueeze(1), 2).repeat(1, len(RSSI), 1, 1).to(device))\n",
        "            loss_clas = FLP_loss(out_clas, HBF_all_4d)\n",
        "\n",
        "            #loss = nn.CrossEntropyLoss()\n",
        "            #input = torch.randn(3, 5, requires_grad=True)\n",
        "            #target = torch.randn(3, 5)\n",
        "            #loss_clas = -loss(out_clas, HBF_all_4d)\n",
        "\n",
        "            # Gradient calculation.\n",
        "            loss_clas.backward()\n",
        "\n",
        "            \n",
        "            # Model weight modification based on the optimizer.\n",
        "            optimizer_m_task.step()\n",
        "          \n",
        "            \n",
        "            # iterate through test dataset\n",
        "            if k == 0 or i % epoch_size == 0:\n",
        "                R_predicted_HBF = []\n",
        "                R_optimum_HBF = []\n",
        "\n",
        "\n",
        "                with th.no_grad():\n",
        "                    for (tchannelR, tchannelI, talpha, tRSSI, tUR, tUI, tAR, tAI, tindex, tWR, tWI, tdeltaR, tdeltaI, tup) in my_testloader:\n",
        "\n",
        "                   \n",
        "                        #e\n",
        "                        testInputs_Reg = Networks_Main_Menu.Inp_MT(tRSSI)\n",
        "\n",
        "                        # Loading the near-optimal digital precoder, CSI (real and imaginary)\n",
        "                        T_wR = tWR.reshape(-1, Us, Nrf).to(device)\n",
        "                        T_wI = tWI.reshape(-1, Us, Nrf).to(device)\n",
        "                        T_channelR = tchannelR.reshape(-1, Us, Mr).to(device)\n",
        "                        T_channelI = tchannelI.reshape(-1, Us, Mr).to(device)\n",
        "\n",
        "                        # Forward pass reg\n",
        "                        Model_m_task.eval()\n",
        "                        start_time = time.time()\n",
        "                        pred1_reg, pred2_reg, pred_class = Model_m_task(testInputs_Reg)\n",
        "\n",
        "                        # find the maximum probability as predication of classification\n",
        "                        _, predicted = th.max(F.softmax(pred_class, 1), 1)\n",
        "\n",
        "                        # mapping in the codebook to find the corresponding analog precoder\n",
        "                        An_Predr = codesr[predicted, :].to(device)\n",
        "                        An_Predi = codesi[predicted, :].to(device)\n",
        "                        w_prer, w_prei = pred1_reg.view(-1, Us, Nrf), pred2_reg.view(-1, Us, Nrf)\n",
        "\n",
        "                        # rate calculation\n",
        "                        # DNN HBF\n",
        "                        R_predicted_HBF.append(criterium_clas_4d.evaluate_rate(w_prer, w_prei, T_channelR, T_channelI, An_Predr, An_Predi))\n",
        "                        \n",
        "                        # near-optimal HBF\n",
        "                        R_optimum_HBF.append(criterium_clas_4d.evaluate_rate(T_wR, T_wI, T_channelR, T_channelI, tAR.to(device), tAI.to(device)))\n",
        "                        end_time = time.time()\n",
        "\n",
        "                        # Calculate inference time\n",
        "                        inference_time = end_time - start_time\n",
        "\n",
        "                # Average over all mini-batches\n",
        "                RATE_Predicted_HBF = sum(R_predicted_HBF) / len(R_predicted_HBF)\n",
        "                RATE_Optimum_HBF = sum(R_optimum_HBF) / len(R_optimum_HBF)\n",
        "                RATE_Ratie_HBF = 100 * RATE_Predicted_HBF / RATE_Optimum_HBF\n",
        "                RATE_Predicted_HBF1.append(RATE_Predicted_HBF)\n",
        "                scheduler_MT.step(RATE_Predicted_HBF)\n",
        "               \n",
        "                print('Iter:==>{:3d}  Loss_Class:{:.3f} Rate_opt_HBF:{:.2f} Rate_pre_HBF:{:.2f} Ratio_HBF:{:.2f}% Inference time:{:.2f}'.\n",
        "                    format(i, loss_clas, RATE_Optimum_HBF, RATE_Predicted_HBF,  RATE_Ratie_HBF, inference_time))\n",
        "                \n",
        "else:\n",
        "    raise Exception('BF_approach is wrong !!')"
      ],
      "metadata": {
        "id": "flaxHtZ-kBAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}